#!/usr/bin/env python

import os
import pickle
import json
import pdb

# Set the paths for the directories and files
projectFolder = "/home/boincadm/projects/boincdocker"
resultsDir = projectFolder + "/results"
agregationFile = projectFolder + "/model/agregation.pkl"
learningFile = projectFolder + "/model/learning.json"
globalWeightsFile = projectFolder + "/model/globalModelWeights.pkl"
errorLogFile = projectFolder + "/model/agregator_error_log.txt"
iterationEnd = 2

try:
    # Get a list of all .pkl files in the results directory
    pklFiles = [f for f in os.listdir(resultsDir) if f.endswith('.pkl')]
    count = len(pklFiles)

    if count > 0:
        # Load the agregation file
        with open(agregationFile, 'rb') as f:
            agregation = pickle.load(f)
        
        # Load each .pkl file from the results directory and update agregation
        for file in pklFiles:
            with open(os.path.join(resultsDir, file), 'rb') as f:
                grad = pickle.load(f)
            
            for l, _ in enumerate(grad):
                agregation[l] += grad[l]       
        
        # Save the updated agregation
        with open(agregationFile, 'wb') as f:
            pickle.dump(agregation, f)
        
        # Update the agregated count in the learning file
        with open(learningFile, 'r+') as f:
            data = json.load(f)
            data['agregatedCount'] += count
            f.seek(0)
            json.dump(data, f, indent=4)
        
        # Delete all .pkl files in the results directory
        for file in pklFiles:
            os.remove(os.path.join(resultsDir, file))
        
        # Check if the updated agregated count is greater than or equal to iterationEnd
        if data['agregatedCount'] >= iterationEnd:
            # Load the global weights file
            with open(globalWeightsFile, 'rb') as f:
                globalWeights = pickle.load(f)
                
            #pdb.set_trace()
            
            # Update global weights using agregation and agregated count from learning file
            for l, _ in enumerate(agregation):
                globalWeights[l] += agregation[l] / data['agregatedCount']
                
            # Save the updated global weights
            with open(globalWeightsFile, 'wb') as f:
                pickle.dump(globalWeights, f)
            
            # Increment iteration number and reset agregated count to 0 in learning file
            data['iterationNumber'] += 1
            data['agregatedCount'] = 0
            
            with open(learningFile, 'w') as f:
                json.dump(data, f, indent=4)
except Exception as e:
    with open(errorLogFile, 'a') as f:
        f.write(f"An error occurred: {e}\n")

# Count .pkl files in /results

# if 'count' > 0, do the agregation
# # load agregation.pkl file
# for loop 'count' times through /results
# # load .pkl file
# # add gradients to agregation variable
# # add +1 to agregated count
# # start new loop

# end, save agregation.pkl

# At the end check if agr_count >= iteration_end (suppose 100)
# if true:
# # apply grads to globalModel.pkl, 
# # start new iteration (add +1 to var), 
# # agr_count = 0, 
# # delete old (all) results in /results
# # write to log accuracy of model with new iteration (initialize model, set weights, checkaccuracy)


# write learning.txt which has
# # weightsNumber (validator reads it) (questionable, because validator would have to read from file every time it's invoked) (maybe bin/start will edit validator to write this var?)
# # iteration_number (assimilator reads it to decide if result is relevant)
# # agregated_count (agregator reads and writes to it)
